{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "\n",
    "from helpers import *\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Decomposition\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Statistical tools\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import linalg\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap, LogNorm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**\n",
    ">\n",
    "> Do not import any other packages or libraries than the ones already provided to you. You can only use the imported packages _after_ they have been imported.\n",
    ">\n",
    "> Write your code between the `BEGIN_TODO` and `END_TODO` markers. Do not change these markers.\n",
    ">\n",
    "> Restart your notebook and run all cells before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment you will learn about different data analysis methods for creating a biking lane detector with the data you have collected in Assignment 1a.This assignment is split into 4 parts. Part 1 will introduce K-means clustering aglorithm and guide you through its implementations and limitations. Part 2 will introduce fuzzy c-means algorithm. Part 3 will explain Gaussian mixture model (GMM), which you will train using the expectation-maximization algorithm. Eventually you are expected to train GMM on your biking data, and use it to assess the biking lane quality of a route in Tilburg.\n",
    "\n",
    "### Learning goals\n",
    "After this assignment you can\n",
    "\n",
    "- implement the K-means algorithm;\n",
    "- explain issues and shortcomings of the K-means algorithm;\n",
    "- implement the fuzzy c-means algorithm;\n",
    "- implement the Gaussian mixture model;\n",
    "- explain how the Gaussian mixture model differs from the previous algorithms;\n",
    "- test your model against real world testing set to detect the biking lane quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: K-means algorithm\n",
    "In this part we will discuss unsupervised machine learning problems and describe how the K-means algorithm can be used to solve these.\n",
    "\n",
    "Unsupervised machine learning problems are problems in which we try to determine some particular structure within a data set. Alternatively, supervised machine learning problems require us to model some kind of input-output mapping. Unsupervised machine learning problems do not have a specified output which we would like to model. Instead we are insterested in making sense of the data and in grouping this data, without knowing beforehand which and how many groups exist.\n",
    "\n",
    "The K-means algorithm can group $N$ data samples of dimension $D$ into $K$ groups or clusters. These clusters can each be characterized by their mean vector: the expected or average value of the points which are assigned to the cluster. The mean vector denoting the center of the $k^{th}$ cluster can be represented as the column vector ${\\bf{\\mu}}^{(k)} = [\\mu_1^{(k)},\\ \\mu_2^{(k)},\\ \\ldots, \\mu_D^{(k)}]^\\top$ and the $n^{th}$ data sample can be represented by the column vector ${\\bf{x}}^{(n)} = [x_1^{(n)},\\ x_2^{(n)},\\ \\ldots, x_D^{(n)}]^\\top$, where the superscript denotes the sample index.\n",
    "\n",
    "The K-means algorithm tries to minimize the (within-cluster) Euclidean squared distance\n",
    "$$J({\\bf{X}}, {\\bf{\\mu}}) = \\frac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^K \\rho_k^{(n)} \\| {\\bf{x}}^{(n)} - {\\bf{\\mu}}^{(k)}\\|^2$$\n",
    "Here $\\rho_k^{(n)}$ is a so-called indicator function that is defined as \n",
    "$$ \\rho_k^{(n)} = \\begin{cases} 1 & \\text{if sample }{\\bf{x}}^{(n)}\\text{ is assigned to cluster }k \\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "This indicator function equals $1$ when the corresponding data point is assigned to the corresponding cluster and $0$ otherwise. The cost function therefore represents the average squared distance with respect to the cluster that a point is assigned to.\n",
    "\n",
    "The algorithm is specified as follows:\n",
    "\n",
    "1. Initialize means ${\\bf{\\mu}}$.\n",
    "2. Assign data points to closest cluster mean (i.e. update $\\rho_k^{(n)}$).\n",
    "3. Calculate new cluster means as the average values of the points that are assigned to it (i.e. update ${\\bf{\\mu}}$).\n",
    "4. Calculate cost function $J({\\bf{X}}, {\\bf{\\mu}})$.\n",
    "5. If not converged, go back to 2 and repeat.\n",
    "\n",
    "Here we will describe the algorithm in words. First the centers of the clusters are initialized. This can be done arbitrarily, but often the centers are set to random (but distinct) samples of the data set.\n",
    "Once the means are set, we assign each data sample to the cluster that is closest to it. In order to do so, we calculate the Euclidean squared distance between a point and all the clusters and find the cluster that is closest to it. We repeat this for all points and we therefore completely specify $\\rho_k^{(n)}$. Once all points have been assigned to a cluster, we look up all points corresponding to a certain cluster and we average these to calculate the new cluster center. We update all cluster means. Then we evaluate the current fit of the clusters on the data by evaluating the cost function. If we still see a significant improvement in the cost function, we repeat updating the assignments and cluster centers and if the cost function seems to have converged, we stop iterating.\n",
    "\n",
    "In this part of the assignment you will implement the K-means algorithm from scratch, starting with the initialization of the cluster means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.1: Initializing cluster centers\n",
    "Consider the function `X = ex3_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `means = initialize_means(X, K)` that accepts the data set ${\\bf{X}}$  and number of clusters $K$ as input and returns a matrix of shape (K x D), representing the vertical concatenation of $K$ transposed mean vectors of dimension $D$. These means should be initialized such that they coincide with *random* samples from the data set, which are always *distinct*. In other words, the means should equal a random subset of the availabe data set, where no means are equal. Also keep in mind that the number of clusters is variable in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the function initialize_means(X, K)\n",
    "\n",
    "\n",
    "    \n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10)\n",
    "plt.scatter(means[:,0], means[:,1], c=\"red\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the clusters have been initialized, it is time to assign points to the closest clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.2: Assign points to clusters\n",
    "Again consider the function `X = ex4_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `rho = assign_data_to_clusters(X, means)` that accepts the data set ${\\bf{X}}$ and matrix of means ${\\bf{\\mu}}$ as input and returns a matrix of shape (N x K), which contains all indicator functions $\\rho_k^{(n)}$. This matrix should be a matrix of only ones and zeros and each row should sum to 1. Note that this is essentially the partition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the function assign_data_to_clusters(X, means)\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# assign point to clusters\n",
    "rho = assign_data_to_clusters(X, means)\n",
    "assert np.allclose(np.sum(rho, axis=1), 1), \"Sum of probabilities for each point does not add up to 1!\"\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], c=\"red\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means have been initialized, the points have been assigned to clusters. Now the cluster centers can be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.3: Update cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again consider the function `X = ex3_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ data vectors of dimension $D$. Create a function `means = update_cluster_centers(X, rho)` that accepts the data set ${\\bf{X}}$ and matrix of indicators $\\rho$ as input and returns a matrix of shape (K x D), which contains the new cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Complete the function update_cluster_centers(X, rho)\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# assign point to clusters\n",
    "rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "# update means\n",
    "means_new = update_cluster_centers(X, rho)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "plt.scatter(means_new[:,0], means_new[:,1], 50, c=\"blue\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Now it is just a matter of combining the previous functions for finalizing the K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.4: Implement K-means algorithm\n",
    "\n",
    "First create a function `J = Kmeans_loss(X, means, rho)` that calculates the within-cluster Euclidean squared distance as defined above. Secondly create the final `means, rho, J = Kmeans(X, K)` function that combines all previous functions to create the K-means algorithm as specified in the the introduction of this part of the assignment. This function returns the final cluster centers, the indicator function and a history of the losses. Save the loss *after* each iteration and stop iterating when the difference in loss does no longer exceed 1e-10. The initial loss based on the randomly initialized means should not be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the Kmeans_loss(X, means, rho) function\n",
    "\n",
    "    \n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Complete the Kmeans(X, K) function\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means, rho, J = Kmeans(X, 3)\n",
    "\n",
    "# plot data\n",
    "_,ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "ax[0].scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "ax[0].scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "ax[1].plot(J)\n",
    "ax[0].grid(), ax[1].grid(), ax[1].set_ylabel(\"cost function\"), ax[1].set_xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.5: Number of clusters\n",
    "In the previous assignment the data had been generated from 3 clusters. In practice the number of clusters is often unknown. In this exercise we will see what happens when we add extra clusters. Run your K-means algorithm on the previous data set for 2 up to and including 10 clusters and save the final value of the loss (i.e. the loss value when the algorithm has converged).\n",
    "\n",
    "Plot the final loss against the number of used clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Plot the Kmeans loss against the number of clusters\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the loss give a good impression of how many clusters can be found in the data set. Please motivate your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Finding an appropriate number of clusters\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.6: Shortcomings of the K-means algorithm\n",
    "Apply the K-means algorithm for the new data set generated by `X = ex36_generate_data()`. Visualise the data and come up with an appropriate number of clusters. Plot the data points in a scatter plot, plot the means as red crosses in the same plot and color the data point according to their assigned cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex36_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Plot clusters for the new data set\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.6\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.7: Clustering biking data\n",
    "Apply the K-means algorithm on the biking data you have collected from assignment 1a, namely the three matrices, combined_features, smooth_features and bumpy_features. Employ PCA with 2 components before you apply K-means. Visualize the data with the cluster centers, and determine the optimum number of clusters for each matrix. For this task, you can use scikit learn packages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  apply k-means and visualize cluster centers and determine optimum number of clusters for each matrix\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO determine optimum number of clusters quantitatively`\n",
    "\n",
    "\n",
    "`#// END_TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO motivate your answer behind the number of clusters`\n",
    "\n",
    "\n",
    "`#// END_TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.7\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fuzzy C-Means Algorithm\n",
    "\n",
    "In this part, we will explore another unsupervised machine learning algorithm known as Fuzzy C-Means (FCM). Unlike K-means, which assigns each data point to a single cluster, FCM allows each data point to belong to multiple clusters with varying degrees of membership.\n",
    "\n",
    "FCM is particularly useful when cluster boundaries are not well-defined. It provides a nuanced understanding of the data by allowing overlapping clusters, which is more appropriate when the data naturally overlaps between clusters.\n",
    "\n",
    "### Overview\n",
    "The Fuzzy C-Means algorithm groups $N$ data samples of dimension $D$ into $C$ clusters, where each cluster is characterized by its cluster center, denoted by the column vector ${\\bf{v}}^{(k)} = [v_1^{(k)},\\ v_2^{(k)},\\ \\ldots, v_D^{(k)}]^\\top$, similar to the mean vector in K-means. However, instead of assigning each data point to a single cluster, FCM assigns a membership value, denoted by $\\mu_k^{(n)}$, to each data point for each cluster. This membership value indicates the degree with which the data point belongs to the cluster.\n",
    "\n",
    "The membership values for the $n^{\\text{th}}$ data sample across all clusters are represented by the vector ${\\bf{\\mu}}^{(n)} = [\\mu_1^{(n)}, \\mu_2^{(n)}, \\ldots, \\mu_C^{(n)}]^\\top$, where $\\mu_k^{(n)}$ is the membership value of the $n^{\\text{th}}$ data sample in the $k^{\\text{th}}$ cluster. These membership values satisfy two conditions:\n",
    "1. $0 \\leq \\mu_k^{(n)} \\leq 1$ for all $k$ and $n$.\n",
    "2. $\\sum_{k=1}^C \\mu_k^{(n)} = 1$ for each $n$.\n",
    "\n",
    "The algorithm aims to minimize the following objective function:\n",
    "\n",
    "$$\n",
    "J({\\bf{X}}, {\\bf{U}}, {\\bf{V}}) = \\frac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^C (\\mu_k^{(n)})^m \\| {\\bf{x}}^{(n)} - {\\bf{v}}^{(k)}\\|^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- ${\\bf{U}}$ represents the matrix of membership values.\n",
    "- ${\\bf{v}}^{(k)}$ represents the cluster center of the $k^{\\text{th}}$ cluster.\n",
    "- $m > 1$ is a weighting exponent that determines the fuzziness of the membership values.\n",
    "\n",
    "### The Fuzzy C-Means Algorithm\n",
    "\n",
    "1. **Initialize the Membership Matrix ${\\bf{U}}$:**\n",
    "   - Randomly initialize the membership values ${\\bf{U}}$ such that they satisfy the constraints mentioned above.\n",
    "\n",
    "2. **Calculate Cluster Centers ${\\bf{v}}$:**\n",
    "   - Compute the cluster center of each cluster as a weighted average of the data points, with weights given by the membership values:\n",
    "   $$\n",
    "   {\\bf{v}}^{(k)} = \\frac{\\sum_{n=1}^N (\\mu_k^{(n)})^m \\cdot {\\bf{x}}^{(n)}}{\\sum_{n=1}^N (\\mu_k^{(n)})^m}\n",
    "   $$\n",
    "\n",
    "3. **Update Membership Values ${\\bf{U}}$:**\n",
    "   - Update the membership values based on the distance between each data point and the cluster centers:\n",
    "   $$\n",
    "   \\mu_k^{(n)} = \\frac{1}{\\sum_{j=1}^C \\left(\\frac{\\|{\\bf{x}}^{(n)} - {\\bf{v}}^{(k)}\\|}{\\|{\\bf{x}}^{(n)} - {\\bf{v}}^{(j)}\\|}\\right)^{\\frac{2}{m-1}}}\n",
    "   $$\n",
    "\n",
    "4. **Calculate the Cost Function $J({\\bf{X}}, {\\bf{U}}, {\\bf{V}})$:**\n",
    "   - Evaluate the current fit of the clusters using the objective function defined earlier.\n",
    "\n",
    "5. **Check for Convergence:**\n",
    "   - If the change in the cost function or the change in cluster centers is below a predefined threshold, stop the iteration. Otherwise, go back to step 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Initialization \n",
    "Initialize the membership partition matrix `U`. Choose also an appropriate value for $m$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ex11_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Initialization\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial Membership Matrix (U):\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Update cluster centers \n",
    "Update the cluster centers `V` based on the current membership matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Update cluster centers\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updated fuzzy Cluster Centers (V):\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Update membership values\n",
    "Update the membership matrix `U` based on the current cluster centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO# update partition matrix\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updated Membership Matrix (U):\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Repeat until convergence\n",
    "employ iterative optimization and repeat until convergence, stop iterations when the convergence threshold ${\\epsilon}\\  < 10^{-5} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Repeat until convergence\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Cluster Centers (V):\")\n",
    "print(V)\n",
    "print(\"Final Membership Matrix (U):\")\n",
    "print(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fuzzy_membership(X, U):\n",
    "    \"\"\"\n",
    "    Plot the data points with fill color based on membership to both clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Data matrix (samples x features)\n",
    "    - U: Membership matrix (samples x clusters)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Define colors and markers\n",
    "    edge_color = 'black'\n",
    "    circle_radius = 0.1  # Increase the radius to make points larger\n",
    "    \n",
    "    # Plot data points for the first cluster\n",
    "    for i in range(X.shape[0]):\n",
    "        membership_value_1 = U[i, 0]\n",
    "        if membership_value_1 > 0.5:\n",
    "            # Higher membership: full blue\n",
    "            circle_1 = plt.Circle((X[i, 0], X[i, 1]), radius=circle_radius, \n",
    "                                  fill=True, alpha=membership_value_1)\n",
    "        else:\n",
    "            # Lower membership: hollow blue\n",
    "            circle_1 = plt.Circle((X[i, 0], X[i, 1]), radius=circle_radius,\n",
    "                                  fill=False, edgecolor='blue', linewidth=1.5, alpha=membership_value_1)\n",
    "        \n",
    "        plt.gca().add_patch(circle_1)\n",
    "    \n",
    "    # Plot data points for the second cluster\n",
    "    for i in range(X.shape[0]):\n",
    "        membership_value_2 = U[i, 1]\n",
    "        if membership_value_2 > 0.5:\n",
    "            # Higher membership: full red\n",
    "            circle_2 = plt.Circle((X[i, 0], X[i, 1]), radius=circle_radius,\n",
    "                                  fill=True, alpha=membership_value_2)\n",
    "        else:\n",
    "            # Lower membership: hollow red\n",
    "            circle_2 = plt.Circle((X[i, 0], X[i, 1]), radius=circle_radius, \n",
    "                                  fill=False, edgecolor='red', linewidth=1.5, alpha=membership_value_2)\n",
    "        \n",
    "        plt.gca().add_patch(circle_2)\n",
    "    \n",
    "    # Plot cluster centers\n",
    "    plt.scatter(V[:, 0], V[:, 1], c='red', marker='x', s=100, label='Cluster Centers')\n",
    "    \n",
    "    # Set plot limits\n",
    "    plt.xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n",
    "    plt.ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Fuzzy C-means clustering')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_fuzzy_membership(X, U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.5: Clustering biking data\n",
    "Apply the FCM algorithm on the biking data you have collected and preprocessed in assignment 1a. Namely the three matrices, combined_features, smooth_features and bumpy_features. Employ PCA with 2 components before you apply FCM. Visualize the data with the cluster centers, and determine the optimum number of clusters. For this task, you can use skfuzzy package. Compare your FCM clusters to K-means clusters and highlight any differences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Clustering biking data with FCM and determining optimum number of clusters\n",
    "\n",
    "#// END_TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO determine optimum number of clusters/qualitatively and compare with k-means`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### End of exercise 2.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gaussian Mixture Modeling\n",
    "The K-Means algorithm is a fast and simple method that works well for many applications. However, due to its simplicity, it isn't suitable for all situations. Fuzzy C-Means (FCM) improves on K-Means by allowing for soft clustering, where data points can belong to multiple clusters with varying degrees of membership. While this flexibility is advantageous, FCM has its own set of limitations. It can be computationally intensive, especially for large datasets, and is sensitive to the initial selection of cluster centers. Additionally, FCM may struggle with outliers, varying cluster sizes, and high-dimensional data, which can lead to suboptimal clustering results.\n",
    "\n",
    "In this part we present another methodology for clustering data, namely through Gaussian mixture modeling. In this approach we do not rely on a deterministic algorithm for determining the cluster means and assignments, but instead we model the data set by fitting a probability density function.\n",
    "\n",
    "We will assume that the data set has been generated from a Gaussian mixture model, which is formally specified as\n",
    "$$ p({\\bf{x}}^{(n)}) = \\sum_{k=1}^K \\rho_k \\mathcal{N}({\\bf{x}}^{(n)} \\mid {\\bf{\\mu}}_k, \\Sigma_k),$$\n",
    "where a data sample ${\\bf{x}}^{(n)}$ is originating from a Gaussian mixture model with $K$ individual Gaussian distributions with means ${\\bf{\\mu}}_k$ and covariance matrices ${\\bf{\\Sigma}}_k$. The mean denotes the center or mode of the Gaussian distribution and the covariance matrix specifies the spread and tilt of the Gaussian distribution. In this model the mixing coefficients $\\rho_k$ specify how much each of the Gaussian distributions contributes in the model. Because the Gaussian mixture model is a probability density function, integrating over ${\\bf{x}}$ should always equal 1. Because the individual Gaussians already satisfy this requirement, the mixing coefficients are constrained by\n",
    "$$ \\sum_{k=1}^K \\rho_k = 1.$$\n",
    "To give some intuition on this model, we give a 1-dimensional example below. Here we model a data set by a mixture of 2 Gaussians. The individual *weighted* Gaussian distributions are colored in blue and the corresponding mixture model distribution is colored in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4_plot_GMM_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this part of the assignment you will implement the so-called Expectation-Maximization (EM) algorithm for learning the Gaussian mixture model. This algorithm consists of two steps, the expectation step (E-step) and the maximization step (M-step). The exact details of the algorithm are beyond the scope of this assignment, but here we will present the update equations for these steps.\n",
    "\n",
    "The EM algorithm works as follows:\n",
    "\n",
    "1. Initialize the means ${\\bf{\\mu}}_k$, covariances $\\Sigma_k$ and mixing coefficients $\\rho_k$. Often the means are initialized using the K-means algorithm. The covariance matrices can be set to identity matrices and the mixing coefficients can be initialized to the fraction of points assigned to the cluster with K-means divided by the total number of samples.\n",
    "2. *Expectation step*: evaluate the responsibilities $\\gamma_{nk}$ using the current parameter values as \n",
    "$$ \\gamma_{nk} = \\frac{\\rho_k \\mathcal{N}({\\bf{x}}_n \\mid {\\bf{\\mu}}_k, \\Sigma_k)}{\\sum_{j=1}^K \\rho_j \\mathcal{N}({\\bf{x}}_n \\mid {\\bf{\\mu}}_j, \\Sigma_j)}$$\n",
    "3. *Maximization step*: re-estimate the parameters using the current responsibilities\n",
    "$$ {\\bf{\\mu}}_k^\\text{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk}{\\bf{x}}_n $$\n",
    "$$ \\Sigma_k^\\text{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk} ({\\bf{x}}_n - {\\bf{\\mu}}_k^\\text{new})({\\bf{x}}_n - {\\bf{\\mu}}_k^\\text{new})^\\top $$\n",
    "$$ \\rho_k = \\frac{N_k}{N} $$\n",
    "where $N$ denotes the number of samples and where\n",
    "$$ N_k = \\sum_{n=1}^N \\gamma_{nk}$$\n",
    "4. Evaluate the log-likelihood\n",
    "$$ \\ln p({\\bf{X}} \\mid {\\bf{\\mu}}, \\Sigma, {\\bf{\\rho}}) = \\sum_{n=1}^N \\ln \\left\\{ \\sum_{k=1}^K \\rho_k \\mathcal{N}({\\bf{x}}^{(n)} \\mid {\\bf{\\mu}}_k, \\Sigma_k)\\right\\}$$\n",
    "\n",
    "It is important to understand what is going on in this algorithm. The responsibilities $\\gamma_{nk}$ are similar to the indicator functions from the K-means algorithm. However, where the K-means algorithm performs a hard clustering (each point can be assigned to only 1 cluster), the Gaussian mixture model allows for a soft clustering (each point can be modeled by both Gaussian distributions, but just to a different extent). The indicator function of the K-means algorithm was one-hot coded, meaning that a point was assigned to 1 cluster only. The responsibilities $\\gamma_{nk}$ specify how likely a data sample ${\\bf{x}}_n$ is to be generated from a cluster. With a Gaussian mixture model a point can therefore be assigned to different extents to multiple clusters. The expectation step calculates these responsibilities and the division in this expression makes sure that all rows sum op to 1.\n",
    "\n",
    "In the maximization step the parameters are updated. Here the contribution of each data sample towards the parameters depends on the corresponding responsibilities. This means that a point that is very likely to have originated from a certain cluster will have a high influence on the statistics of that cluster. The variable $N_k$ specifies how many points are located to a certain Gaussian distribution. Because this parameter is the summation over the individual responsibilities, $N_k$ is not forced to be an integer.\n",
    "\n",
    "The log-likelihood is a cost function which takes the variances and uncertainties in our model into account. It describes the probability of that data set being generated from a Gaussian mixture model. To prevent numerical instability we commonly use the log-likelihood instead of the normal likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: Initialize clusters\n",
    "Consider the function from the previous part `X = ex46_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `means, covs, rho = initialize_GMM(X, K)` that accepts the data set ${\\bf{X}}$ as input and returns the following in this order:\n",
    "- `means`: a matrix of size (K x D) that contains the initial cluster means, as a vertical concatenation of the transposed mean vectors. These means should be initialized using the previously written K-means algorithm.\n",
    "- `covs`: a matrix of size (K x D x D) that contains the covariance matrices of the initial clusters. Each matrix `covs[k,:,:]` represents the covariance matrix of the $k^\\text{th}$ cluster. Initialize these covariance matrices as identity matrices.\n",
    "- `rho`: a vector of length K that contains the mixing coefficients as specified above. Initialize this vector based on the indicator function returned by the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the initialize_GMM(X, K) function\n",
    "    \n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "X = ex36_generate_data()\n",
    "\n",
    "# initialize GMM\n",
    "means, covs, rho = initialize_GMM(X, 2)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2: Expectation step\n",
    "Create a function `gamma = expectation_step(X, means, covs, rho)` that accepts the data set, means, covariances and mixing coefficients with dimensions specified above. This function should perform the expectation step and should return the calculated responsibilities as defined above as a matrix of size (N x K) where each row corresponds to the assignment fraction of a sample amongst the different clusters. Make sure this matrix is properly normalized such that the elements in each row add up to 1. Use the `multivariate_normal` function that has been imported from `scipy.stats` at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Complete the expectation_step(X, means, covs, rho) function\n",
    "\n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = expectation_step(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.3: Maximization step\n",
    "Create a function `means, covs, rho = maximization_step(X, gamma)` that accepts the data set and responsibilities with dimensions specified above. This function should perform the maximization step and should return the new means, covariances and mixing coefficients with dimensions as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the maximization_step(X, gamma) function\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximization step\n",
    "means, covs, rho = maximization_step(X, gamma)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.4: Log-likelihood calculation\n",
    "Create a function `J = loglikelihood(X, means, covs, rho)` that accepts the data set, means, covariance matrices and mixing coefficients with dimensions specified above. This function should calculate and return the log-likelihood of the data under the specified Gaussian mixture model. Use the definition as specified in the beginning of Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the loglikelihood(X, means, covs, rho) function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.5: Gaussian mixture modeling\n",
    "Now that all the subfunctions have been defined it is time to tie them together and to form a function which does the Gaussian mixture modelling. Create a function `means, covs, rho, gamma, J = GMM_modeling(X, K, nr_iterations)` that does the following:\n",
    "\n",
    "1. Initialize the parameters of the Gaussian mixture model.\n",
    "2. Performs `nr_iterations` iterations of the following:\n",
    "    1. Perform the expectation step.\n",
    "    2. Perform the maximization step.\n",
    "    3. Calculate the log-likelihood.\n",
    "3. returns the parameters and a vector of saved values of the log-likelihood.\n",
    "\n",
    "The function should return all the parameters of the trained Gaussian mixture model, containing the final means, covariance matrices, mixing coefficients, responsibilities and a vector containing all calculated values of the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the GMM_modeling(X, K, nr_iterations) function\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GMM\n",
    "means, covs, rho, gamma, J = GMM_modeling(X, 2, 10)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(J)\n",
    "plt.grid(), plt.xlabel(\"iteration\"), plt.ylabel(\"log-likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6: Gaussian mixture modeling with biking data\n",
    "Now that you have an end to end understanding of GMM pipeline, implement your hardcoded methodology on your biking data. Do not forget to perform PCA. For this exercise, use your `combined_features` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the initialize_GMM(X, K) function\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your implementation against sklearn implementation\n",
    "\n",
    "def plot_gmm_contours(gmm, X, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Define the grid\n",
    "    x = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 300)\n",
    "    y = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 300)\n",
    "    X_grid, Y_grid = np.meshgrid(x, y)\n",
    "    XY = np.vstack([X_grid.ravel(), Y_grid.ravel()]).T\n",
    "    \n",
    "    # Compute the negative log-likelihood of the GMM over the grid\n",
    "    Z = -gmm.score_samples(XY)\n",
    "    Z = Z.reshape(X_grid.shape)\n",
    "\n",
    "    # Plot the contour\n",
    "    CS = ax.contourf(X_grid, Y_grid, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10), cmap='Blues')\n",
    "    CB = plt.colorbar(CS, ax=ax, shrink=0.8, extend='both')\n",
    "    \n",
    "    # Predict the component assignments for each data point\n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    # Plot data points with colors based on component assignment\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, edgecolor='k', alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Negative Log-Likelihood Predicted by GMM')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # Add a legend to the plot\n",
    "    handles, labels_legend = scatter.legend_elements(num=len(np.unique(labels)))\n",
    "    ax.legend(handles, [f'Component {i+1}' for i in np.unique(labels)], title=\"Components\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Perform PCA\n",
    "pca_object = PCA(n_components=2)  # Replace ICA with PCA\n",
    "data_pca = pca_object.fit_transform(combined_features)  # Apply PCA to combined_features\n",
    "\n",
    "# Fit GMM on PCA-transformed data\n",
    "gmm_pca = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
    "gmm_pca.fit(data_pca)\n",
    "\n",
    "# Plot GMM contours for PCA data\n",
    "plot_gmm_contours(gmm_pca, data_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.6\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 3.7: Number of GMM components\n",
    "Proceeding with the sklearn implementation, employ different elbow based quantitative techniques to decide optimum number of components. Employ your `combined_features` matrix and sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO determine number of GMM components\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO determine number of GMM components`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`#// END_TODO `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.7\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8: Building the bumpy lane detector\n",
    "You made it this far. We are almost there. Now, train two gaussian mixture models, one using your smooth data `smooth_features` and one using your `bumpy_features`. For this assignment you are provided with a testing data taken from biking in Tilburg for 17 minutes on biking lane with diverse features. You are requested to employ your models to determine which segments of the road are bumpy and which segments are smooth. You must employ your preprocessing skills and provide your judgement on road segments for each 30 second segment. \n",
    "for example : bumpy, smooth, smooth, bumpy, smooth, etc... until end of the road!\n",
    "Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO train two GMM models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Preproces Tilburg data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Classify Tilburg biking lane\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data you’ve collected and the predictions you've made, summarize your opinion on the overall quality of Tilburg’s biking lanes. In no more than 100 words, provide your general impression of the road conditions, highlighting whether the lanes are predominantly smooth or bumpy. Additionally, offer any recommendations for potential improvements based on the results of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// Tilburg lane quality\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.8\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure to restart this notebook and to rerun all cells before submission to check whether all code runs properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Assignment 1b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
